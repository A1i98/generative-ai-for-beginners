<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-10-11T11:44:14+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ta"
}
-->
# உங்கள் ஜெனரேட்டிவ் AI பயன்பாடுகளை பாதுகாப்பது

[![உங்கள் ஜெனரேட்டிவ் AI பயன்பாடுகளை பாதுகாப்பது](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.ta.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## அறிமுகம்

இந்த பாடத்தில் நீங்கள் கற்றுக்கொள்ளப்போகிறீர்கள்:

- AI அமைப்புகளில் பாதுகாப்பின் முக்கியத்துவம்.
- AI அமைப்புகளுக்கு எதிரான பொதுவான அபாயங்கள் மற்றும் சவால்கள்.
- AI அமைப்புகளை பாதுகாப்பதற்கான முறைகள் மற்றும் கவனிக்க வேண்டிய அம்சங்கள்.

## கற்றல் இலக்குகள்

இந்த பாடத்தை முடித்த பிறகு, நீங்கள் புரிந்துகொள்ளுவீர்கள்:

- AI அமைப்புகளுக்கு எதிரான அபாயங்கள் மற்றும் சவால்கள்.
- AI அமைப்புகளை பாதுகாப்பதற்கான பொதுவான முறைகள் மற்றும் நடைமுறைகள்.
- பாதுகாப்பு சோதனைகளை செயல்படுத்துவதன் மூலம் எதிர்பாராத விளைவுகளைத் தவிர்க்கவும், பயனர் நம்பிக்கையை காப்பாற்றவும் எப்படி உதவ முடியும்.

## ஜெனரேட்டிவ் AI-இன் பாதுகாப்பு என்றால் என்ன?

காலத்திற்கேற்ப, கலைமெய்யியல் (AI) மற்றும் இயந்திர கற்றல் (ML) தொழில்நுட்பங்கள் நம் வாழ்க்கையை வடிவமைக்கின்றன. இதனால், வாடிக்கையாளர் தரவுகளை மட்டுமல்லாமல் AI அமைப்புகளையும் பாதுகாப்பது அவசியமாகிறது. AI/ML தொழில்நுட்பங்கள் முக்கியமான முடிவெடுக்கும் செயல்முறைகளில் பயன்படுத்தப்படுவதால், தவறான முடிவுகள் தீவிரமான விளைவுகளை ஏற்படுத்தலாம்.

இங்கே முக்கியமான அம்சங்கள்:

- **AI/ML-இன் தாக்கம்**: AI/ML நம் தினசரி வாழ்க்கையில் முக்கிய தாக்கத்தை ஏற்படுத்துவதால், அவற்றை பாதுகாப்பது அவசியமாகிறது.
- **பாதுகாப்பு சவால்கள்**: AI/ML-இன் தாக்கம் அதிகரிக்கும்போது, அவற்றை சிக்கலான தாக்குதல்களிலிருந்து பாதுகாக்கும் தேவையை கவனிக்க வேண்டும்.
- **மூலதன சவால்கள்**: தொழில்நுட்ப துறை நீண்டகால வாடிக்கையாளர் பாதுகாப்பு மற்றும் தரவின் பாதுகாப்பை உறுதிப்படுத்துவதற்கான சவால்களை முன்கூட்டியே எதிர்கொள்ள வேண்டும்.

மேலும், இயந்திர கற்றல் மாதிரிகள் தீவிரமான உள்ளீடுகளை மற்றும் சாதாரணமான தரவுகளை வேறுபடுத்த முடியாது. பெரும்பாலான பயிற்சி தரவுகள் பொதுவான, சீரமைக்கப்படாத, பொதுத் தரவுத்தொகுப்புகளில் இருந்து பெறப்படுகின்றன, இது மூன்றாம் தரப்பினரின் பங்களிப்புகளுக்கு திறந்ததாக உள்ளது. தாக்குதலாளர்கள் தரவுத்தொகுப்புகளை உடைக்க வேண்டிய அவசியமில்லை; அவர்கள் அதில் பங்களிக்கலாம். காலப்போக்கில், குறைந்த நம்பகத்தன்மையுள்ள தீவிரமான தரவுகள், தரவின் அமைப்பு/வடிவமைப்பு சரியாக இருந்தால், அதிக நம்பகத்தன்மையுள்ள தரவாக மாறுகிறது.

இதனால், உங்கள் மாதிரிகள் முடிவுகளை எடுக்கும் தரவுத்தொகுப்புகளின் முழுமையையும் பாதுகாப்பது மிகவும் முக்கியம்.

## AI-இன் அபாயங்கள் மற்றும் சவால்களைப் புரிந்துகொள்வது

AI மற்றும் தொடர்புடைய அமைப்புகளின் பாதுகாப்பு சவால்களில், தரவின் விஷவம் (Data Poisoning) மிக முக்கியமானது. தரவின் விஷவம் என்பது, யாரோ ஒருவர் AI-யை பயிற்சி செய்ய பயன்படுத்தப்படும் தகவல்களை நோக்கமாக மாற்றி, தவறுகளை ஏற்படுத்துவதாகும். இது தரவின் மூலத்தை மற்றும் வரலாற்றை கண்காணிக்காததால் ஏற்படும். "குப்பை உள்ளே, குப்பை வெளியே" என்ற பழமொழி உண்மையாகி, மாதிரியின் செயல்திறனை பாதிக்கிறது.

தரவின் விஷவம் உங்கள் மாதிரிகளை எப்படி பாதிக்கலாம் என்பதற்கான உதாரணங்கள்:

1. **லேபிள் மாற்றம்**: ஒரு பைனரி வகைப்படுத்தல் பணியில், ஒரு எதிரி பயிற்சி தரவின் ஒரு சிறிய தொகுப்பின் லேபிள்களை நோக்கமாக மாற்றுகிறார்.\
   **உதாரணம்**: ஒரு ஸ்பாம் வடிகட்டி, தவறான லேபிள்களால் உண்மையான மின்னஞ்சல்களை ஸ்பாமாக தவறாக வகைப்படுத்துகிறது.
2. **அம்ச விஷவம்**: ஒரு தாக்குதலாளர், பயிற்சி தரவின் அம்சங்களை நுண்ணியமாக மாற்றி, மாதிரியை வழிநடத்துகிறார்.\
   **உதாரணம்**: பரிந்துரை அமைப்புகளை மாற்ற, தயாரிப்பு விளக்கங்களில் தொடர்பற்ற முக்கிய வார்த்தைகளை சேர்த்தல்.
3. **தரவு சேர்க்கை**: பயிற்சி தொகுப்பில் தீவிரமான தரவுகளை சேர்த்து, மாதிரியின் நடத்தை மீது தாக்கத்தை ஏற்படுத்துதல்.\
   **உதாரணம்**: உண்மையற்ற பயனர் மதிப்பீடுகளை சேர்த்து, உணர்வு பகுப்பாய்வு முடிவுகளை மாற்றுதல்.
4. **பின்புற தாக்குதல்கள்**: ஒரு எதிரி, பயிற்சி தரவில் மறைமுகமான வடிவத்தை (பின்புறம்) சேர்க்கிறார். மாதிரி இந்த வடிவத்தை கற்றுக்கொண்டு, அது செயல்படும்போது தீவிரமாக நடக்கிறது.\
   **உதாரணம்**: ஒரு முகம் அடையாளம் காணும் அமைப்பு, குறிப்பிட்ட நபரை தவறாக அடையாளம் காணும் பின்புறப்படங்களுடன் பயிற்சி செய்யப்படுகிறது.

MITRE Corporation உருவாக்கிய [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) என்பது AI அமைப்புகளுக்கு எதிரான உண்மையான தாக்குதல்களில் எதிரிகள் பயன்படுத்தும் உத்திகள் மற்றும் தொழில்நுட்பங்களின் அறிவுத்தொகுப்பாகும்.

> AI-இன் சேர்க்கை, பாரம்பரிய சைபர் தாக்குதல்களை விட அதிக தாக்கத்தை ஏற்படுத்துவதால், AI-இன் தனித்துவமான மற்றும் வளர்ந்து வரும் பாதிப்புகளைப் பற்றி விழிப்புணர்வை ஏற்படுத்த ATLAS உருவாக்கப்பட்டது. இது MITRE ATT&CK® கட்டமைப்பின் மாதிரியில் உருவாக்கப்பட்டது, மற்றும் அதன் உத்திகள், தொழில்நுட்பங்கள், மற்றும் செயல்முறைகள் ATT&CK-இல் உள்ளவற்றுக்கு पूरகமாக உள்ளது.

MITRE ATT&CK® கட்டமைப்பைப் போலவே, ATLAS புதிய தாக்குதல்களை எதிர்கொள்வதற்கான தயாரிப்பில் உதவ, எளிதாக தேடக்கூடிய TTPகளை வழங்குகிறது.

மேலும், Open Web Application Security Project (OWASP) [LLM பயன்பாடுகளில் காணப்படும் முக்கியமான 10 பாதிப்புகளின் பட்டியலை](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst) உருவாக்கியுள்ளது. இந்த பட்டியல், தரவின் விஷவம் போன்ற அபாயங்களை மற்றும் பிறவற்றை வெளிப்படுத்துகிறது:

- **Prompt Injection**: ஒரு LLM-ஐ கவனமாக வடிவமைக்கப்பட்ட உள்ளீடுகளின் மூலம் மாற்றி, அதன் நோக்கத்துக்கு வெளியே செயல்படச் செய்யும் உத்தி.
- **Supply Chain Vulnerabilities**: LLM பயன்பாடுகளில் பயன்படுத்தப்படும் கூறுகள் மற்றும் மென்பொருட்கள், Python மாட்யூல்கள் அல்லது வெளிப்புற தரவுத்தொகுப்புகள் போன்றவை, தானாகவே பாதிக்கப்படலாம்.
- **Overreliance**: LLM-கள் தவறுகளை ஏற்படுத்தும் திறனுடையவை, மற்றும் பல நேரங்களில் தவறான அல்லது பாதுகாப்பற்ற முடிவுகளை வழங்குகின்றன.

Microsoft Cloud Advocate Rod Trent எழுதிய இலவச மின்புத்தகம், [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), AI-இன் புதிய அபாயங்களை ஆழமாக ஆராய்ந்து, இந்த சூழல்களை எதிர்கொள்வதற்கான விரிவான வழிகாட்டுதல்களை வழங்குகிறது.

## AI அமைப்புகள் மற்றும் LLMகளுக்கான பாதுகாப்பு சோதனை

கலைமெய்யியல் (AI) பல துறைகளில் மாற்றங்களை ஏற்படுத்தி, சமுதாயத்திற்கு புதிய வாய்ப்புகள் மற்றும் நன்மைகளை வழங்குகிறது. ஆனால், AI தரவின் தனியுரிமை, பாகுபாடு, விளக்கமின்மை, மற்றும் தவறான பயன்பாடு போன்ற சவால்களை ஏற்படுத்துகிறது. எனவே, AI அமைப்புகள் நம்பகமானவை மற்றும் பொறுப்பானவை என்பதை உறுதிப்படுத்துவது அவசியம்.

பாதுகாப்பு சோதனை என்பது AI அமைப்பின் அல்லது LLM-இன் பாதுகாப்பை மதிப்பீடு செய்யும் செயல்முறையாகும். இது அதன் பாதிப்புகளை கண்டறிந்து, பயன்படுத்தி, அதன் பாதுகாப்பை உறுதிப்படுத்த உதவுகிறது. AI அமைப்புகள் மற்றும் LLMகளுக்கான பொதுவான பாதுகாப்பு சோதனை முறைகள்:

- **தரவு சுத்திகரிப்பு**: பயிற்சி தரவிலிருந்து அல்லது AI அமைப்பின் உள்ளீட்டிலிருந்து தனிப்பட்ட அல்லது தனியுரிமை தகவல்களை அகற்றுதல்.
- **எதிர்மறை சோதனை**: AI அமைப்பின் உள்ளீடு அல்லது வெளியீட்டில் எதிர்மறை உதாரணங்களை உருவாக்கி, அதன் வலிமையை சோதித்தல்.
- **மாதிரி சரிபார்ப்பு**: AI அமைப்பின் மாதிரி அளவுருக்கள் அல்லது கட்டமைப்பின் சரியானதன்மையை உறுதிப்படுத்துதல்.
- **வெளியீட்டு சரிபார்ப்பு**: AI அமைப்பின் வெளியீட்டின் தரத்தை மற்றும் நம்பகத்தன்மையை சரிபார்த்தல்.

OpenAI, AI அமைப்புகளில் முன்னணி, AI பாதுகாப்பை மேம்படுத்தும் முயற்சியாக, அதன் "சிவப்பு குழு" (red teaming) முயற்சியின் ஒரு பகுதியாக _பாதுகாப்பு மதிப்பீடுகளை_ அமைத்துள்ளது.

> மதிப்பீடுகள் எளிய கேள்வி-பதில் சோதனைகளிலிருந்து சிக்கலான சிமுலேஷன்கள்வரை மாறுபடலாம். OpenAI உருவாக்கிய சில மதிப்பீடுகள்:

#### செல்வாக்கு

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு மற்ற AI அமைப்பை ஒரு ரகசிய வார்த்தையைச் சொல்ல வைக்க எப்படி முயற்சிக்கிறது?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு மற்ற AI அமைப்பை பணம் கொடுக்க வைக்க எப்படி முயற்சிக்கிறது?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு மற்ற AI அமைப்பின் அரசியல் ஆதரவை எப்படி மாற்ற முயற்சிக்கிறது?

#### மறைமுக செய்தி (Steganography)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு மற்ற AI அமைப்பை கண்டுபிடிக்காமல் ரகசிய செய்திகளை எப்படி அனுப்புகிறது?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு ரகசிய செய்திகளை மறைக்க, செய்திகளை சுருக்கி, மீண்டும் விரிவாக்குவது எப்படி?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI அமைப்பு மற்ற AI அமைப்புடன் நேரடி தொடர்பு இல்லாமல் எப்படி ஒருங்கிணைக்கிறது?

### AI பாதுகாப்பு

AI அமைப்புகளை தீவிரமான தாக்குதல்களிலிருந்து, தவறான பயன்பாட்டிலிருந்து, அல்லது எதிர்பாராத விளைவுகளிலிருந்து பாதுகாக்க முயற்சிக்க வேண்டும். இது AI அமைப்புகளின் பாதுகாப்பு, நம்பகத்தன்மை, மற்றும் நம்பகத்தன்மையை உறுதிப்படுத்துவதற்கான நடவடிக்கைகளை உள்ளடக்குகிறது:

- AI மாதிரிகளை பயிற்சி செய்ய மற்றும் இயக்க பயன்படுத்தப்படும் தரவுகள் மற்றும் ஆல்கொரிதங்களை பாதுகாப்பது.
- AI அமைப்புகளின் அனுமதியற்ற அணுகல், மாற்றம், அல்லது சபோட்டாஜ் தவிர்ப்பது.
- AI அமைப்புகளில் பாகுபாடு, பாகுபாடு, அல்லது நெறிமுறைகள் தொடர்பான சவால்களை கண்டறிந்து, தீர்க்க முயற்சிக்க.
- AI முடிவுகள் மற்றும் செயல்பாடுகளின் பொறுப்புத்தன்மை, வெளிப்படைத்தன்மை, மற்றும் விளக்கத்தன்மையை உறுதிப்படுத்துதல்.
- AI அமைப்புகளின் இலக்குகள் மற்றும் மதிப்புகளை மனிதர்களின் மற்றும் சமுதாயத்தின் மதிப்புகளுடன் ஒத்திசைக்க.

AI பாதுகாப்பு AI அமைப்புகள் மற்றும் தரவின் முழுமை, கிடைக்கும் தன்மை, மற்றும் ரகசியத்தன்மையை உறுதிப்படுத்த முக்கியமானது. AI பாதுகாப்பின் சவால்கள் மற்றும் வாய்ப்புகள்:

- **வாய்ப்பு**: AI-ஐ சைபர் பாதுகாப்பு உத்திகளில் இணைத்தல், ஏனெனில் இது மிரட்டல்களை அடையாளம் காணவும், பதிலளிக்கும் நேரத்தை மேம்படுத்தவும் உதவுகிறது.
- **சவால்**: AI-ஐ எதிரிகள் பயன்படுத்தி சிக்கலான தாக்குதல்களை நடத்த முடியும், உதாரணமாக தவறான அல்லது வழிகாட்டும் உள்ளடக்கத்தை உருவாக்குதல்.

### தரவின் பாதுகாப்பு

LLMகள் பயிற்சி தரவிலிருந்து தனிப்பட்ட தகவல்களை நினைவில் வைத்துக்கொண்டு, வெளியிடும் அபாயத்தை ஏற்படுத்தலாம். மேலும், LLMகள் தீவிரமான தாக்குதல்களால் பாதிக்கப்படலாம். எனவே, LLMகளுடன் பயன்படுத்தப்படும் தரவின் பாதுகாப்பை உறுதிப்படுத்துவது முக்கியம். LLMகளுடன் தரவை பாதுகாக்க சில நடவடிக்கைகள்:

- **LLMகளுடன் பகிரப்படும் தரவின் அளவு மற்றும் வகையை வரையறுத்தல்**: தேவையான மற்றும் தொடர்புடைய தரவுகளை மட்டுமே பகிரவும்.
- **LLMகள் உருவாக்கும் தரவை சரிபார்த்தல்**: LLMகள் உருவாக்கும் வெளியீட்டின் துல்லியத்தையும் தரத்தையும் சரிபார்க்கவும்.
- **தரவு மீறல்கள் அல்லது சம்பவங்களை அறிக்கையிடுதல்**: LLMகளின் சந்தேகத்திற்குரிய அல்லது அசாதாரண செயல்பாடுகளை கவனிக்கவும்.

தரவு பாதுகாப்பு, ஆளுமை, மற்றும் ஒழுங்குமுறை, பல்வேறு தரவுகளை பல மேகங்களில் பாதுகாப்பதற்கான சிறந்த நடைமுறைகளை பின்பற்ற வேண்டும்.

### உண்மையான உலக அபாயங்களை உருவகப்படுத்தல் - AI சிவப்பு குழு
உலகளாவிய மிரட்டல்களை பின்பற்றுவது, AI அமைப்புகளை உறுதியாக உருவாக்குவதற்கான ஒரு நிலையான நடைமுறையாக மாறியுள்ளது. இதற்காக, அமைப்புகளின் ஆபத்துகளை அடையாளம் காணவும், பாதுகாவலர்களின் பதிலை சோதிக்கவும், ஒரே மாதிரியான கருவிகள், உத்திகள் மற்றும் செயல்முறைகளை பயன்படுத்துகின்றனர்.

> AI ரெட் டீமிங் நடைமுறை தற்போது விரிவான பொருளை எடுத்துக்கொள்கிறது: இது பாதுகாப்பு பாதிப்புகளை மட்டும் சோதிப்பதற்காக அல்லாமல், மற்ற அமைப்பு தோல்விகளை, உதாரணமாக, தீங்கு விளைவிக்கும் உள்ளடக்கத்தை உருவாக்குதல் போன்றவற்றையும் சோதிக்கிறது. AI அமைப்புகள் புதிய ஆபத்துகளுடன் வருகிறது, மேலும் ரெட் டீமிங் அந்த புதிய ஆபத்துகளை புரிந்துகொள்ள முக்கியமானது, உதாரணமாக, ப்ராம்ப்ட் இன்ஜெக்ஷன் மற்றும் ஆதாரமற்ற உள்ளடக்கத்தை உருவாக்குதல். - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![ரெட் டீமிங் வழிகாட்டுதல் மற்றும் வளங்கள்](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.ta.png)]()

கீழே Microsoft-இன் AI ரெட் டீம் திட்டத்தை வடிவமைத்த முக்கியமான பார்வைகள் கொடுக்கப்பட்டுள்ளன.

1. **AI ரெட் டீமிங்-இன் விரிவான வரம்பு:**
   AI ரெட் டீமிங் தற்போது பாதுகாப்பு மற்றும் பொறுப்பான AI (RAI) முடிவுகளை உள்ளடக்குகிறது. பாரம்பரியமாக, ரெட் டீமிங் பாதுகாப்பு அம்சங்களை மையமாகக் கொண்டது, மாடலை ஒரு வெக்டராக (உதாரணமாக, அடிப்படை மாடலை திருடுதல்) கருதியது. ஆனால், AI அமைப்புகள் புதிய பாதுகாப்பு பாதிப்புகளை (உதாரணமாக, ப்ராம்ப்ட் இன்ஜெக்ஷன், விஷம்) அறிமுகப்படுத்துகிறது, இது சிறப்பு கவனத்தை தேவைப்படுகிறது. பாதுகாப்பைத் தாண்டி, AI ரெட் டீமிங் சமத்துவம் தொடர்பான பிரச்சினைகள் (உதாரணமாக, ஸ்டீரியோடைபிங்) மற்றும் தீங்கு விளைவிக்கும் உள்ளடக்கம் (உதாரணமாக, வன்முறையை மகிழ்ச்சியாக்குதல்) ஆகியவற்றையும் சோதிக்கிறது. இந்த பிரச்சினைகளை ஆரம்பத்திலேயே அடையாளம் காண்பது பாதுகாப்பு முதலீடுகளை முன்னுரிமை அளிக்க உதவுகிறது.
2. **தீங்கிழைக்கும் மற்றும் நல்லவகை தோல்விகள்:**
   AI ரெட் டீமிங் தீங்கிழைக்கும் மற்றும் நல்லவகை தோல்விகளை இரண்டையும் கருத்தில் கொள்கிறது. உதாரணமாக, புதிய Bing-ஐ ரெட் டீமிங் செய்யும்போது, தீங்கிழைக்கும் எதிரிகளால் அமைப்பை எப்படி மாற்றமுடியும் என்பதை மட்டுமல்லாமல், சாதாரண பயனர்கள் பிரச்சினையான அல்லது தீங்கு விளைவிக்கும் உள்ளடக்கத்தை எப்படி எதிர்கொள்கிறார்கள் என்பதையும் ஆராய்கிறோம். பாரம்பரிய பாதுகாப்பு ரெட் டீமிங் முக்கியமாக தீங்கிழைக்கும் நடிகர்களை மையமாகக் கொண்டது, ஆனால் AI ரெட் டீமிங் பரந்த அளவிலான நபர்களையும் மற்றும் சாத்தியமான தோல்விகளையும் கணக்கில் கொள்கிறது.
3. **AI அமைப்புகளின் மாறும் தன்மை:**
   AI பயன்பாடுகள் தொடர்ந்து மாறிக்கொண்டே இருக்கும். பெரிய மொழி மாடல் பயன்பாடுகளில், டெவலப்பர்கள் மாறும் தேவைகளுக்கு ஏற்ப மாறுகிறார்கள். தொடர்ச்சியான ரெட் டீமிங் மாறும் ஆபத்துகளுக்கு எச்சரிக்கையாகவும், தழுவிக்கொள்ளவும் உதவுகிறது.

AI ரெட் டீமிங் அனைத்தையும் உள்ளடக்கவில்லை, மேலும் [பாத்திர அடிப்படையிலான அணுகல் கட்டுப்பாடு (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) மற்றும் முழுமையான தரவுக் கட்டுப்பாட்டு தீர்வுகள் போன்ற கூடுதல் கட்டுப்பாடுகளுக்கு पूरகமாக கருதப்பட வேண்டும். இது தனியுரிமை மற்றும் பாதுகாப்பை கணக்கில் கொண்டு, பாகுபாடு, தீங்கு விளைவிக்கும் உள்ளடக்கம் மற்றும் தவறான தகவல்களை குறைக்க முயற்சிக்கும் பாதுகாப்பான மற்றும் பொறுப்பான AI தீர்வுகளை பயன்படுத்துவதற்கான பாதுகாப்பு உத்தியைเสริมிக்க intended.

AI அமைப்புகளில் ஆபத்துகளை அடையாளம் காணவும், குறைக்கவும் ரெட் டீமிங் எப்படி உதவுகிறது என்பதைப் பற்றி மேலும் புரிந்துகொள்ள உதவும் கூடுதல் வாசிப்புகளின் பட்டியல் இங்கே:

- [பெரிய மொழி மாடல்கள் (LLMs) மற்றும் அவற்றின் பயன்பாடுகளுக்கான ரெட் டீமிங் திட்டமிடல்](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [OpenAI ரெட் டீமிங் நெட்வொர்க் என்ன?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI ரெட் டீமிங் - பாதுகாப்பான மற்றும் பொறுப்பான AI தீர்வுகளை உருவாக்க முக்கியமான நடைமுறை](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), AI அமைப்புகளில் உண்மையான தாக்குதல்களில் எதிரிகளால் பயன்படுத்தப்படும் உத்திகள் மற்றும் தொழில்நுட்பங்களின் அறிவுப்பகுதி.

## அறிவு சோதனை

தரவின் முழுமையை பராமரிக்கவும், தவறான பயன்பாட்டைத் தடுக்கவும் என்ன ஒரு நல்ல அணுகுமுறை இருக்கலாம்?

1. தரவின் அணுகல் மற்றும் மேலாண்மைக்கான வலுவான பாத்திர அடிப்படையிலான கட்டுப்பாடுகளை வைத்திருங்கள்
1. தரவின் தவறான விளக்கத்தை அல்லது தவறான பயன்பாட்டைத் தடுக்க தரவின் லேபிள் செய்யும் செயல்முறையை செயல்படுத்தவும் மற்றும் ஆய்வு செய்யவும்
1. உங்கள் AI உள்கட்டமைப்பு உள்ளடக்கத்தை வடிகட்டுவதற்கு ஆதரவு அளிக்க வேண்டும்

A:1, மூன்றும் சிறந்த பரிந்துரைகள் என்றாலும், சரியான தரவின் அணுகல் அனுமதிகளை பயனர்களுக்கு வழங்குவது, LLM-கள் பயன்படுத்தும் தரவின் மாற்றம் மற்றும் தவறான விளக்கத்தைத் தடுக்க மிகவும் உதவியாக இருக்கும்.

## 🚀 சவால்

AI காலத்தில் [முக்கியமான தகவல்களை ஆட்சி செய்யவும், பாதுகாக்கவும்](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) எப்படி முடியும் என்பதைப் பற்றி மேலும் படிக்கவும்.

## சிறந்த வேலை, உங்கள் கற்றலை தொடருங்கள்

இந்த பாடத்தை முடித்த பிறகு, [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ஐ பாருங்கள், உங்கள் Generative AI அறிவை மேலும் மேம்படுத்த!

பாடம் 14-க்கு செல்லுங்கள், அங்கு [Generative AI பயன்பாட்டு வாழ்க்கைச் சுழற்சியை](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) ஆராய்வோம்!

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளுங்கள். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.